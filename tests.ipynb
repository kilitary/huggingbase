{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install  transformers[tf-cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T00:36:38.233129500Z",
     "start_time": "2023-10-13T00:36:33.347027300Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, RadioButtons\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"conversational\")\n",
    "classifier([Conversation(\"what is rfc number for rj42?\")])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: ru_instruct_gpt4/default\n",
      "Found cached dataset ru_instruct_gpt4 (C:/Users/deconf/.cache/huggingface/datasets/lksy___ru_instruct_gpt4/default/0.0.1/b2cfa50a05a4e665ead398fce73de411b1fd8791d282289a2c79eede7097b1b4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72541f31011a4c34961a6d3365908f58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'full_output'],\n",
      "        num_rows: 15056\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'lksy/ru_instruct_gpt4'. Make sure that:\n\n- 'lksy/ru_instruct_gpt4' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'lksy/ru_instruct_gpt4' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    358\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresolved_config_file\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 359\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    360\u001B[0m             \u001B[0mconfig_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dict_from_json_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresolved_config_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32mc:\\Windows\\Temp\\ipykernel_19076\\1699932744.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"  \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0md\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0md\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mclassifier\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"lksy/ru_instruct_gpt4\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtask\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'text-generation'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0mc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mConversation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"document about protocol of system\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmark_processed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\pipelines.py\u001B[0m in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, framework, **kwargs)\u001B[0m\n\u001B[0;32m   2714\u001B[0m             \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2715\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2716\u001B[1;33m             \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2717\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2718\u001B[0m     \u001B[1;31m# Instantiate config if needed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\tokenization_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m         \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"config\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 216\u001B[1;33m             \u001B[0mconfig\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    217\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    218\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m\"bert-base-japanese\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\configuration_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    308\u001B[0m             \u001B[1;33m{\u001B[0m\u001B[1;34m'foo'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m         \"\"\"\n\u001B[1;32m--> 310\u001B[1;33m         \u001B[0mconfig_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPretrainedConfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_config_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    311\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    312\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m\"model_type\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mconfig_dict\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\configuration_utils.py\u001B[0m in \u001B[0;36mget_config_dict\u001B[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[0;32m    366\u001B[0m                 \u001B[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    367\u001B[0m             )\n\u001B[1;32m--> 368\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mEnvironmentError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    369\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    370\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mJSONDecodeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: Can't load config for 'lksy/ru_instruct_gpt4'. Make sure that:\n\n- 'lksy/ru_instruct_gpt4' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'lksy/ru_instruct_gpt4' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "conversation_1 = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation_2 = Conversation(\"What's the last book you have read?\")\n",
    "\n",
    "conversational_pipeline([conversation_1, conversation_2])\n",
    "\n",
    "conversation_1.add_user_input(\"Is it an action movie?\")\n",
    "conversation_2.add_user_input(\"What is the genre of this book?\")\n",
    "\n",
    "conversational_pipeline([conversation_1, conversation_2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T23:19:23.350401800Z",
     "start_time": "2023-10-12T23:19:21.174399600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"generator\") to str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mc:\\Windows\\Temp\\ipykernel_20820\\3644757119.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mpipe\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'text-generation'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'gpt2-medium'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mgenerated_characters\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mout\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpipe\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'out={out}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mgenerated_characters\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"generated_text\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mF:\\huggingface\\venv\\lib\\site-packages\\transformers\\pipelines.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, return_tensors, return_text, clean_up_tokenization_spaces, prefix, *args, **generate_kwargs)\u001B[0m\n\u001B[0;32m    870\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    871\u001B[0m                 \u001B[0mprefix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprefix\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 872\u001B[1;33m                 \u001B[0minputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_parse_and_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprefix\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mprompt_text\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_special_tokens\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    873\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    874\u001B[0m                 \u001B[1;31m# set input_ids to None to allow empty prompt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: can only concatenate str (not \"generator\") to str"
     ]
    }
   ],
   "source": [
    "def data():\n",
    "    for i in range(1000):\n",
    "        yield f\"My example {i}\"\n",
    "\n",
    "\n",
    "pipe = pipeline('text-generation', model='gpt2-medium')\n",
    "generated_characters = 0\n",
    "for out in pipe(data()):\n",
    "    print(f'out={out}')\n",
    "    generated_characters += len(out[0][\"generated_text\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T00:39:47.363688300Z",
     "start_time": "2023-10-13T00:39:39.131488200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'The man worked as a security guard in a military'},\n {'generated_text': 'The man worked as a salesman in Mexico and eventually'},\n {'generated_text': 'The man worked as a supervisor at the department for'},\n {'generated_text': 'The man worked as a cleaner for the same corporation'},\n {'generated_text': 'The man worked as a barman and was involved'}]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "\n",
    "set_seed(43)\n",
    "generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n",
    "\n",
    "set_seed(42)\n",
    "generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T00:38:28.173935Z",
     "start_time": "2023-10-13T00:38:18.555174100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': \"Hello, I'm a language model, I'm a language. I'm a compiler, I'm a parser, I'm a server process. I\"},\n {'generated_text': \"Hello, I'm a language model, and I'd like to join an existing team. What can I do to get started?\\n\\nI'd\"},\n {'generated_text': \"Hello, I'm a language model, why does my code get created? Can't I just copy it? But why did my code get created when\"},\n {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI'm a functional language. Is it hard? A little, yes. But\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I need to give me objects from which I can get\"}]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T00:37:10.011659700Z",
     "start_time": "2023-10-13T00:36:58.951519600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
